<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Jialiang Xu</title> <meta name="author" content="Jialiang Xu"> <meta name="description" content="The personal website of Jialiang Xu. "> <meta name="keywords" content="Jialiang Xu"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8C%B2&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://liamjxu.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/Jialiang_CV.pdf">Curriculum Vitae </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Jialiang</span> Xu <a href="https://en-audio.howtopronounce.com/e3b8a8f287c0ed9538ced20a837134b3.mp3" rel="external nofollow noopener" target="_blank"><img src="assets/img/sound_icon_gray.png" title="How to pronounce?" alt="How to pronounce?" class="sound-icon"></a> </h1> <p class="desc"></p> </header> <article> <div class="profile float-right"> <figure> <picture> <img src="/assets/img/my_pic.jpg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="my_pic.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="clearfix"> <p>Hi! I’m Jialiang. I am currently pursuing my M.S. in Computer Science at <a href="https://cs.stanford.edu/" rel="external nofollow noopener" target="_blank">Stanford University</a>, where I have the privilege of working under the guidance of Prof. <a href="https://suif.stanford.edu/~lam/" rel="external nofollow noopener" target="_blank">Monica Lam</a>, focusing on LLM-related research. Before embarking on my graduate studies at Stanford, I had the opportunity to be mentored by Prof. <a href="http://blender.cs.illinois.edu/hengji.html" rel="external nofollow noopener" target="_blank">Heng Ji</a> and work closely with Prof. <a href="https://abdelzaher.cs.illinois.edu/" rel="external nofollow noopener" target="_blank">Tarek Abdelzaher</a>, at <a href="https://illinois.edu/" rel="external nofollow noopener" target="_blank">the University of Illinois at Urbana-Champaign</a>. From June 2021 to July 2022, I worked as a research intern at <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/" rel="external nofollow noopener" target="_blank">Microsoft Research</a>, supervised by Dr. <a href="https://www.microsoft.com/en-us/research/people/mezho/" rel="external nofollow noopener" target="_blank">Mengyu Zhou</a>. From June to September in 2023, I worked as an applied scientist intern at <a href="https://www.amazon.jobs/en/business_categories/alexa-and-amazon-devices" rel="external nofollow noopener" target="_blank">Amazon Alexa</a>, supervised by Dr. <a href="https://www.linkedin.com/in/mehdimohamad/" rel="external nofollow noopener" target="_blank">Mohamad Mehdi</a>.</p> <p>My research primarily aims to enable effective, robust, and interpretable Large Language Models (LLMs) for reasoning, understanding, and analysis on human language. In addition to LLMs, I am also passionate about building LLM-based real-world Autonomous Agents that are rigorous, tractable, and interactive.</p> </div> <div class="news"> <h2>News</h2> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Aug 15, 2024</th> <td> Our paper led by the amazing Chi, <a href="https://blender.cs.illinois.edu/paper/lmsteer2024.pdf" rel="external nofollow noopener" target="_blank">LM-Steer: Word Embeddings Are Steers for Language Models</a>, has won the Outstanding Paper Award at ACL 2024! 📢 </td> </tr> <tr> <th scope="row">Aug 10, 2024</th> <td> I will be travelling to Bangkok, Thailand from Aug 11-16 for ACL 2024! If you are too, let’s meet up and have a coffee together! <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">Jun 17, 2024</th> <td> I have started my return internship at Amazon AGI! 🍌 </td> </tr> <tr> <th scope="row">May 23, 2024</th> <td> Our proposal, Cross Lingual Multiple Perspective News, has won <a href="https://brown.columbia.edu/2024-25-magic-grants/" rel="external nofollow noopener" target="_blank">Brown Institute Magic Grant 2024-2025</a>. 📸 </td> </tr> <tr> <th scope="row">May 16, 2024</th> <td> Two papers <a href="https://arxiv.org/pdf/2305.12798" rel="external nofollow noopener" target="_blank">[1]</a> <a href="https://arxiv.org/pdf/2406.00562" rel="external nofollow noopener" target="_blank">[2]</a> accepted by ACL 2024. 📃 </td> </tr> <tr> <th scope="row">Mar 13, 2024</th> <td> <a href="https://openreview.net/forum?id=DoSQeeVlUO" rel="external nofollow noopener" target="_blank">One paper</a> accepted by <a href="https://2024.naacl.org/" rel="external nofollow noopener" target="_blank">NAACL 2024</a>! This is the first research project I participated in at Stanford. 📃 </td> </tr> <tr> <th scope="row">Sep 15, 2023</th> <td> I have wrapped up my internship at <a href="https://developer.amazon.com/en-US/alexa/" rel="external nofollow noopener" target="_blank">Alexa AI, Amazon</a>! A huge thanks to the APEX leadership and recruiters for an incredible summer. Onward! 👋🍌 </td> </tr> <tr> <th scope="row">May 2, 2023</th> <td> <a href="https://arxiv.org/abs/2209.00946" rel="external nofollow noopener" target="_blank">One paper</a> accepted by <a href="https://2023.aclweb.org/" rel="external nofollow noopener" target="_blank">ACL 2023</a>! This is the first research project I participated in at Microsoft. 📃 </td> </tr> <tr> <th scope="row">Mar 18, 2023</th> <td> I received my MSCS admission letter from Stanford, starting Autumn Quarter 2023-2024! Thank you Stanford! 🌲 </td> </tr> <tr> <th scope="row">Dec 17, 2022</th> <td> I graduated with a B.Sc. degree with <a href="https://ece.illinois.edu/academics/ugrad/honors-programs" rel="external nofollow noopener" target="_blank">Highest Honors</a>! 🎓 </td> </tr> <tr> <th scope="row">Dec 10, 2022</th> <td> I will be presenting our poster in the Atrium, ADNEC at 9:00am, Dec. 10! Please feel free to stop by and ask questions if you are interested! <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">Oct 20, 2022</th> <td> I got 337 + 5.0 in the GRE test! <img class="emoji" title=":dart:" alt=":dart:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3af.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">Oct 6, 2022</th> <td> My first-author paper <a href="https://arxiv.org/abs/2211.07455" rel="external nofollow noopener" target="_blank">DNC framework</a> is accepted to <a href="https://2022.emnlp.org/" rel="external nofollow noopener" target="_blank">EMNLP 2022</a>! See you in Abu Dhabi! <img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">Jul 1, 2021</th> <td> My internship at <a href="https://www.microsoft.com/en-us/research/group/data-knowledge-intelligence/" rel="external nofollow noopener" target="_blank">DKI group, Microsoft Research Asia</a> began! <img class="emoji" title=":man_technologist:" alt=":man_technologist:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f468-1f4bb.png" height="20" width="20"> </td> </tr> </table> </div> </div> <div class="publications"> <h2>Selected Publications</h2> <p> (For full publication list please refer to my <a href="https://scholar.google.com/citations?user=S_mgVngAAAAJ" target="_blank" rel="external nofollow noopener">Google Scholar</a> page. "*" denotes equal contribution)</p> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge" style="background-color:#DC3535"><a href="https://2024.aclweb.org//" rel="external nofollow noopener" target="_blank">ACL 2024</a></abbr><abbr class="badge" style="background-color:#FFAF00">Outstanding</abbr> </div> <div id="han2024word" class="col-sm-8"> <div class="title">Word Embeddings Are Steers for Language Models</div> <div class="author"> Chi Han, <em><b>Jialiang Xu</b></em>, Manling Li, Yi Fung, Chenkai Sun, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Nan Jiang, Tarek Abdelzaher, Heng Ji' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="award" style="font-size: smaller; font-weight: 350; color:var(--global-theme-color)"><b>(ACL 2024 Outstanding Paper Award)</b></div> <div class="periodical"> 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://arxiv.org/abs/2305.12798" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://arxiv.org/pdf/2305.12798" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Language models (LMs) automatically learn word embeddings during pre-training on language corpora. Although word embeddings are usually interpreted as feature vectors for individual words, their roles in language model generation remain underexplored. In this work, we theoretically and empirically revisit output word embeddings and find that their linear transformations are equivalent to steering language model generation styles. We name such steers LM-Steers and find them existing in LMs of all sizes. It requires learning parameters equal to 0.2% of the original LMs’ size for steering each style. On tasks such as language model detoxification and sentiment control, LM-Steers can achieve comparable or superior performance compared with state-of-the-art controlled generation methods while maintaining a better balance with generation quality. The learned LM-Steer serves as a lens in text styles: it reveals that word embeddings are interpretable when associated with language model generations and can highlight text spans that most indicate the style differences. An LM-Steer is transferrable between different language models by an explicit form calculation. One can also continuously steer LMs simply by scaling the LM-Steer or compose multiple LM-Steers by adding their transformations.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">han2024word</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Word Embeddings Are Steers for Language Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Han, Chi and Xu, Jialiang and Li, Manling and Fung, Yi and Sun, Chenkai and Jiang, Nan and Abdelzaher, Tarek and Ji, Heng}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2305.12798}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}</span><span class="p">,</span>
  <span class="na">acc_type</span> <span class="p">=</span> <span class="s">{Outstanding}</span><span class="p">,</span>
  <span class="na">award</span> <span class="p">=</span> <span class="s">{ACL 2024 Outstanding Paper Award}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge" style="background-color:#DC3535"><a href="https://2024.naacl.org/" rel="external nofollow noopener" target="_blank">NAACL 2024</a></abbr><abbr class="badge" style="background-color:#B4B4B8">Findings</abbr> </div> <div id="liu2024suql" class="col-sm-8"> <div class="title">SUQL: Conversational Search over Structured and Unstructured Data with Large Language Models</div> <div class="author"> Shicheng Liu, <em><b>Jialiang Xu</b></em>, Wesley Tjangnaka, Sina Semnani, Chen Jie Yu, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Monica Lam' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://arxiv.org/abs/2311.09818" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://arxiv.org/pdf/2311.09818.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Many knowledge sources consist of both structured information such as relational databases as well as unstructured free text. Building a conversational interface to such data sources is challenging. This paper introduces SUQL, Structured and Unstructured Query Language, the first formal executable representation that naturally covers compositions of structured and unstructured data queries. Specifically, it augments SQL with several free-text primitives to form a precise, succinct, and expressive representation. This paper also presents a conversational search agent based on large language models, including a few-shot contextual semantic parser for SUQL. To validate our approach, we introduce a dataset consisting of crowdsourced questions and conversations about real restaurants. Over 51% of the questions in the dataset require both structured and unstructured data, suggesting that it is a common phenomenon. We show that our few-shot conversational agent based on SUQL finds an entity satisfying all user requirements 89.3% of the time, compared to just 65.0% for a strong and commonly used baseline. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">liu2024suql</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{SUQL}: Conversational Search over Structured and Unstructured Data with Large Language Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu, Shicheng and Xu, Jialiang and Tjangnaka, Wesley and Semnani, Sina and Yu, Chen Jie and Lam, Monica}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=DoSQeeVlUO}</span><span class="p">,</span>
  <span class="na">acc_type</span> <span class="p">=</span> <span class="s">{Findings}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#DC3535"><a href="https://2022.emnlp.org/" rel="external nofollow noopener" target="_blank">EMNLP 2022</a></abbr></div> <div id="https://doi.org/10.48550/arxiv.2211.07455" class="col-sm-8"> <div class="title">Towards Robust Numerical Question Answering: Diagnosing Numerical Capabilities of NLP Systems</div> <div class="author"> <em><b>Jialiang Xu</b></em>, Mengyu Zhou, Xinyi He, Shi Han, and Dongmei Zhang</div> <div class="periodical"> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://arxiv.org/abs/2211.07455" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://arxiv.org/pdf/2211.07455.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/pdf/poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="/assets/pdf/dnc_slides.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>Numerical Question Answering is the task of answering questions that require numerical capabilities. Previous works introduce general adversarial attacks to Numerical Question Answering, while not systematically exploring numerical capabilities specific to the topic. In this paper, we propose to conduct numerical capability diagnosis on a series of Numerical Question Answering systems and datasets. A series of numerical capabilities are highlighted, and corresponding dataset perturbations are designed. Empirical results indicate that existing systems are severely challenged by these perturbations. E.g., Graph2Tree experienced a 53.83% absolute accuracy drop against the “Extra” perturbation on ASDiv-a, and BART experienced 13.80% accuracy drop against the “Language” perturbation on the numerical subset of DROP. As a counteracting approach, we also investigate the effectiveness of applying perturbations as data augmentation to relieve systems’ lack of robust numerical capabilities. With experiment analysis and empirical studies, it is demonstrated that Numerical Question Answering with robust numerical capabilities is still to a large extent an open question. We discuss future directions of Numerical Question Answering and summarize guidelines on future dataset collection and system design.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex">  <span class="c">doi = {10.48550/ARXIV.2211.07455},</span>
  <span class="c">author = {Xu, Jialiang and Zhou, Mengyu and He, Xinyi and Han, Shi and Zhang, Dongmei},</span>
  <span class="c">keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},</span>
  <span class="c">title = {Towards Robust Numerical Question Answering: Diagnosing Numerical Capabilities of NLP Systems},</span>
  <span class="c">publisher = {arXiv},</span>
  <span class="c">year = {2022},</span>
  <span class="c">copyright = {arXiv.org perpetual, non-exclusive license},</span>
<span class="c">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%78%6A%6C@%73%74%61%6E%66%6F%72%64.%65%64%75" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=S_mgVngAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/liamjxu" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/xjl" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/liamjxu" title="Twitter" rel="external nofollow noopener" target="_blank"><i class="fab fa-twitter"></i></a> <a href="/feed.xml" title="RSS Feed"><i class="fas fa-rss-square"></i></a> </div> <div class="contact-note"> The easiest way to reach me would be through email since I regularly check my mailbox every day. </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Jialiang Xu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Profile photo By Ke Xu. Last updated: August 18, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-9N1F8DT3CC"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-9N1F8DT3CC");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>