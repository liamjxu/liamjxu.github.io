@misc{https://doi.org/10.48550/arxiv.2211.07455,
  doi = {10.48550/ARXIV.2211.07455},  
  url = {https://arxiv.org/abs/2211.07455},  
  author = {Xu, Jialiang and Zhou, Mengyu and He, Xinyi and Han, Shi and Zhang, Dongmei},  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},  
  title = {Towards Robust Numerical Question Answering: Diagnosing Numerical Capabilities of NLP Systems},  
  publisher = {arXiv},  
  year = {2022},  
  copyright = {arXiv.org perpetual, non-exclusive license},
  selected = true,
  abstract = {Numerical Question Answering is the task of answering questions that require numerical capabilities. Previous works introduce general adversarial attacks to Numerical Question Answering, while not systematically exploring numerical capabilities specific to the topic. In this paper, we propose to conduct numerical capability diagnosis on a series of Numerical Question Answering systems and datasets. A series of numerical capabilities are highlighted, and corresponding dataset perturbations are designed. Empirical results indicate that existing systems are severely challenged by these perturbations. E.g., Graph2Tree experienced a 53.83% absolute accuracy drop against the ``Extra'' perturbation on ASDiv-a, and BART experienced 13.80% accuracy drop against the ``Language'' perturbation on the numerical subset of DROP. As a counteracting approach, we also investigate the effectiveness of applying perturbations as data augmentation to relieve systems' lack of robust numerical capabilities. With experiment analysis and empirical studies, it is demonstrated that Numerical Question Answering with robust numerical capabilities is still to a large extent an open question. We discuss future directions of Numerical Question Answering and summarize guidelines on future dataset collection and system design.},
  arxiv = {2211.07455},
  bibtex_show = true,
  pdf = {https://arxiv.org/pdf/2211.07455.pdf},
  code = {https://github.com/microsoft/NumberDiagnosis},
  poster = {poster.pdf},
  slides = {dnc_slides.pdf},
  abbr = {EMNLP 2022}
}


@misc{https://doi.org/10.48550/arxiv.2212.02691,
  doi = {10.48550/ARXIV.2212.02691},  
  url = {https://arxiv.org/abs/2212.02691},  
  author = {Han*, Hongwei and Xu*, Jialiang and Zhou, Mengyu and Shao, Yijia and Han, Shi and Zhang, Dongmei},  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},  
  title = {LUNA: Language Understanding with Number Augmentations on Transformers via Number Plugins and Pre-training},  
  publisher = {arXiv},  
  year = {2022},  
  copyright = {arXiv.org perpetual, non-exclusive license},
  selected = true,
  abstract = {Transformers are widely used in NLP tasks. However, current approaches to leveraging transformers to understand language expose one weak spot: Number understanding. In some scenarios, numbers frequently occur, especially in semi-structured data like tables. But current approaches to rich-number tasks with transformer-based language models abandon or lose some of the numeracy information - e.g., breaking numbers into sub-word tokens - which leads to many number-related errors. In this paper, we propose the LUNA framework which improves the numerical reasoning and calculation capabilities of transformer-based language models. With the number plugin of NumTok and NumBed, LUNA represents each number as a whole to model input. With number pre-training, including regression loss and model distillation, LUNA bridges the gap between number and vocabulary embeddings. To the best of our knowledge, this is the first work that explicitly injects numeracy capability into language models using Number Plugins. Besides evaluating toy models on toy tasks, we evaluate LUNA on three large-scale transformer models (RoBERTa, BERT, TabBERT) over three different downstream tasks (TATQA, TabFact, CrediTrans), and observe the performances of language models are constantly improved by LUNA. The augmented models also improve the official baseline of TAT-QA (EM: 50.15 -> 59.58) and achieve SOTA performance on CrediTrans (F1 = 86.17).},
  arxiv = {2212.02691},
  bibtex_show = true,
  pdf = {https://arxiv.org/pdf/2212.02691.pdf},
  abbr = {ArXiv}
}


@misc{https://doi.org/10.48550/arxiv.2209.00946,
  doi = {10.48550/ARXIV.2209.00946},  
  url = {https://arxiv.org/abs/2209.00946},  
  author = {He, Xinyi and Zhou, Mengyu and Xu, Jialiang and Lv, Xiao and Li, Tianle and Shao, Yijia and Han, Shi and Yuan, Zejian and Zhang, Dongmei},  
  keywords = {Databases (cs.DB), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},  
  title = {Inferring Tabular Analysis Metadata by Infusing Distribution and Knowledge Information},  
  publisher = {arXiv},  
  year = {2022},  
  copyright = {arXiv.org perpetual, non-exclusive license},
  selected = true,
  abstract = {Many data analysis tasks heavily rely on a deep understanding of tables (multi-dimensional data). Across the tasks, there exist comonly used metadata attributes of table fields / columns. In this paper, we identify four such analysis metadata: Measure/dimension dichotomy, common field roles, semantic field type, and default aggregation function. While those metadata face challenges of insufficient supervision signals, utilizing existing knowledge and understanding distribution. To inference these metadata for a raw table, we propose our multi-tasking Metadata model which fuses field distribution and knowledge graph information into pre-trained tabular models. For model training and evaluation, we collect a large corpus (~582k tables from private spreadsheet and public tabular datasets) of analysis metadata by using diverse smart supervisions from downstream tasks. Our best model has accuracy = 98%, hit rate at top-1 > 67%, accuracy > 80%, and accuracy = 88% for the four analysis metadata inference tasks, respectively. It outperforms a series of baselines that are based on rules, traditional machine learning methods, and pre-trained tabular models. Analysis metadata models are deployed in a popular data analysis product, helping downstream intelligent features such as insights mining, chart / pivot table recommendation, and natural language QA...},
  arxiv = {2209.00946},
  bibtex_show = true,
  pdf = {https://arxiv.org/pdf/2209.00946.pdf},
  abbr = {ArXiv}
}
