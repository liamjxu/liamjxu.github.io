<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Jialiang Xu </title> <meta name="author" content="Jialiang Xu"> <meta name="description" content="The personal website of Jialiang Xu. "> <meta name="keywords" content="Jialiang Xu"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8C%B2&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://liamjxu.github.io/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Curriculum Vitae </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Jialiang</span> Xu <a href="https://en-audio.howtopronounce.com/e3b8a8f287c0ed9538ced20a837134b3.mp3" rel="external nofollow noopener" target="_blank"> <img src="/assets/img/sound_icon_gray.png" title="How to pronounce?" alt="How to pronounce?" class="sound-icon"> </a> </h1> <p class="desc"></p> </header> <article> <div class="profile float-right"> <figure> <picture> <img src="/assets/img/my_pic.jpg?36cd759bc3bf029712979cc0cd01da36" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="my_pic.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="clearfix"> <p>I am a recent M.S. graduate in Computer Science from <a href="https://cs.stanford.edu/" rel="external nofollow noopener" target="_blank">Stanford University</a>, where I worked with <a href="https://cs.stanford.edu/~pliang/" rel="external nofollow noopener" target="_blank">Prof. Percy Liang</a>.</p> <p>My research centers on large language models (LLMs), particularly, identifying model weaknesses <a href="https://aclanthology.org/2022.emnlp-main.542.pdf" rel="external nofollow noopener" target="_blank">[1]</a><a href="https://aclanthology.org/2024.emnlp-main.1147.pdf" rel="external nofollow noopener" target="_blank">[2]</a><a href="https://arxiv.org/pdf/2405.18740" rel="external nofollow noopener" target="_blank">[3]</a>, improving model capabilities <a href="https://arxiv.org/pdf/2212.02691.pdf" rel="external nofollow noopener" target="_blank">[1]</a><a href="https://aclanthology.org/2024.acl-long.864.pdf" rel="external nofollow noopener" target="_blank">[2]</a>, and building useful systems <a href="https://arxiv.org/pdf/2311.09818.pdf" rel="external nofollow noopener" target="_blank">[1]</a><a href="https://arxiv.org/pdf/2406.00562" rel="external nofollow noopener" target="_blank">[2]</a><a href="https://aclanthology.org/2024.findings-emnlp.938.pdf" rel="external nofollow noopener" target="_blank">[3]</a>. I am also a strong proponent for democratizing model evaluation. I contributed to the maintenance and extension of <a href="https://github.com/stanford-crfm/helm" rel="external nofollow noopener" target="_blank">HELM</a>, an open source Python framework for holistic, reproducible and transparent evaluation of foundation models. Most recently, I led the development of <a href="https://crfm.stanford.edu/2025/03/20/helm-capabilities.html" rel="external nofollow noopener" target="_blank">HELM Capabilities</a>, the latest flagship benchmark of the HELM suite.</p> <p>Through assistantships and internships, I have had the privilege to collaborate with many exceptional researchers and engineers, including: <a href="https://yifanmai.com" rel="external nofollow noopener" target="_blank">Yifan Mai</a>, <a href="https://cs.stanford.edu/people/jure/" rel="external nofollow noopener" target="_blank">Prof. Jure Leskovec</a>, and <a href="https://suif.stanford.edu/~lam/" rel="external nofollow noopener" target="_blank">Prof. Monica Lam</a> at Stanford University; <a href="http://michaelmoor.me" rel="external nofollow noopener" target="_blank">Prof. Michael Moor</a> now at ETH Zurich; <a href="http://blender.cs.illinois.edu/hengji.html" rel="external nofollow noopener" target="_blank">Prof. Heng Ji</a> and <a href="https://abdelzaher.cs.illinois.edu/" rel="external nofollow noopener" target="_blank">Prof. Tarek Abdelzaher</a> at the University of Illinois at Urbana-Champaign; <a href="https://www.stevens.edu/profile/dzhang42" rel="external nofollow noopener" target="_blank">Prof. Denghui Zhang</a> and <a href="https://ottovonxu.github.io" rel="external nofollow noopener" target="_blank">Prof. Zhaozhuo Xu</a> at Stevens Institute of Technology; <a href="https://www.linkedin.com/in/mehdimohamad/" rel="external nofollow noopener" target="_blank">Dr. Mohamad Mehdi</a>, <a href="https://chetannaik.github.io" rel="external nofollow noopener" target="_blank">Chetan Naik</a>, and <a href="https://www.amazon.science/author/jane-you" rel="external nofollow noopener" target="_blank">Dr. Jane You</a> at Amazon Alexa; and <a href="https://www.microsoft.com/en-us/research/people/mezho/" rel="external nofollow noopener" target="_blank">Dr. Mengyu Zhou</a>, <a href="https://www.microsoft.com/en-us/research/people/shihan/" rel="external nofollow noopener" target="_blank">Shi Han</a>, and <a href="https://www.microsoft.com/en-us/research/people/dongmeiz/" rel="external nofollow noopener" target="_blank">Dr. Dongmei Zhang</a> at Microsoft Research.</p> </div> <h2> <a href="/news/" style="color: inherit">News</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Jun 15, 2025</th> <td> I graduated from <a href="https://www.stanford.edu" rel="external nofollow noopener" target="_blank">Stanford University</a> with a M.Sc. degree in Computer Science! üå≤ </td> </tr> <tr> <th scope="row" style="width: 20%">Mar 20, 2025</th> <td> We released <a href="https://crfm.stanford.edu/2025/03/20/helm-capabilities.html" rel="external nofollow noopener" target="_blank">HELM Capabilities</a>, the latest flagship benchmark in the HELM suite that evaluates models capability-by-capability. üîç </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 21, 2024</th> <td> Finished my internship at <a href="https://developer.amazon.com/en-US/alexa/" rel="external nofollow noopener" target="_blank">Amazon Alexa</a>! A huge thanks to the APEX leadership and recruiters for yet another incredible summer! üçå </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 20, 2024</th> <td> Two papers <a href="https://aclanthology.org/2024.emnlp-main.1147.pdf" rel="external nofollow noopener" target="_blank">[1]</a><a href="https://aclanthology.org/2024.findings-emnlp.938.pdf" rel="external nofollow noopener" target="_blank">[2]</a> accepted to <a href="https://2024.emnlp.org/" rel="external nofollow noopener" target="_blank">EMNLP 2024</a>! See you in Miami! üìÉ </td> </tr> <tr> <th scope="row" style="width: 20%">Aug 15, 2024</th> <td> Our paper led by the amazing Chi, <em><a href="https://blender.cs.illinois.edu/paper/lmsteer2024.pdf" rel="external nofollow noopener" target="_blank">LM-Steer: Word Embeddings Are Steers for Language Models</a></em>, has won the Outstanding Paper Award at <a href="https://2024.aclweb.org" rel="external nofollow noopener" target="_blank">ACL 2024</a>! üèÖ </td> </tr> <tr> <th scope="row" style="width: 20%">Aug 10, 2024</th> <td> I will be travelling to Bangkok, Thailand from Aug 11-16 for <a href="https://2024.aclweb.org" rel="external nofollow noopener" target="_blank">ACL 2024</a>! If you are too, let‚Äôs meet up and have a coffee together! <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20"> </td> </tr> <tr> <th scope="row" style="width: 20%">May 23, 2024</th> <td> Our proposal, Cross Lingual Multiple Perspective News, has won <a href="https://brown.columbia.edu/2024-25-magic-grants/" rel="external nofollow noopener" target="_blank">Brown Institute Magic Grant 2024-2025</a>. üì∏ </td> </tr> <tr> <th scope="row" style="width: 20%">May 16, 2024</th> <td> Two papers <a href="https://arxiv.org/pdf/2305.12798" rel="external nofollow noopener" target="_blank">[1]</a> <a href="https://arxiv.org/pdf/2406.00562" rel="external nofollow noopener" target="_blank">[2]</a> accepted by <a href="https://2024.aclweb.org" rel="external nofollow noopener" target="_blank">ACL 2024</a>! See you in Bangkok! üìÉ </td> </tr> <tr> <th scope="row" style="width: 20%">Mar 13, 2024</th> <td> One paper<a href="https://openreview.net/forum?id=DoSQeeVlUO" rel="external nofollow noopener" target="_blank">[1]</a> accepted by <a href="https://2024.naacl.org/" rel="external nofollow noopener" target="_blank">NAACL 2024</a>! This is my first research project at Stanford. üìÉ </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 15, 2023</th> <td> I have wrapped up my internship at <a href="https://developer.amazon.com/en-US/alexa/" rel="external nofollow noopener" target="_blank">Amazon Alexa</a>! A huge thanks to the APEX leadership and recruiters for an incredible summer. Onward! üçå </td> </tr> <tr> <th scope="row" style="width: 20%">May 02, 2023</th> <td> One paper <a href="https://arxiv.org/abs/2209.00946" rel="external nofollow noopener" target="_blank">[1]</a> accepted by <a href="https://2023.aclweb.org/" rel="external nofollow noopener" target="_blank">ACL 2023</a>! This is the first research project I participated in at Microsoft. üìÉ </td> </tr> <tr> <th scope="row" style="width: 20%">Dec 17, 2022</th> <td> I graduated from <a href="https://illinois.edu" rel="external nofollow noopener" target="_blank">UIUC</a> with a B.Sc. degree with <a href="https://ece.illinois.edu/academics/ugrad/honors-programs" rel="external nofollow noopener" target="_blank">Highest Honors</a> in Computer Engineering! üåΩ </td> </tr> <tr> <th scope="row" style="width: 20%">Dec 10, 2022</th> <td> I will be presenting our poster at <a href="https://2022.emnlp.org/" rel="external nofollow noopener" target="_blank">EMNLP 2022</a> in the Atrium, ADNEC at 9:00 am on Dec 10! Please stop by! <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20"> </td> </tr> <tr> <th scope="row" style="width: 20%">Oct 06, 2022</th> <td> One paper <a href="https://arxiv.org/abs/2211.07455" rel="external nofollow noopener" target="_blank">[1]</a> accepted to <a href="https://2022.emnlp.org/" rel="external nofollow noopener" target="_blank">EMNLP 2022</a>! See you in Abu Dhabi! <img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20"> </td> </tr> <tr> <th scope="row" style="width: 20%">Jun 24, 2022</th> <td> Finished my internship at <a href="https://www.microsoft.com/en-us/research/group/data-knowledge-intelligence/" rel="external nofollow noopener" target="_blank">DKI group, Microsoft Research Asia</a>! <img class="emoji" title=":man_technologist:" alt=":man_technologist:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f468-1f4bb.png" height="20" width="20"> </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">Selected Publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#DC3535"> <a href="https://2024.emnlp.org/" rel="external nofollow noopener" target="_blank">EMNLP 2024</a> </abbr> </div> <div id="xu-etal-2024-llms" class="col-sm-8"> <div class="title">Do LLMs Know to Respect Copyright Notice?</div> <div class="author"> <em>Jialiang Xu</em>, Shenglan Li, Zhaozhuo Xu, and Denghui Zhang </div> <div class="periodical"> <em>In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing</em>, Nov 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.18653/v1/2024.emnlp-main.1147" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://aclanthology.org/2024.emnlp-main.1147.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Prior study shows that LLMs sometimes generate content that violates copyright. In this paper, we study another important yet underexplored problem, i.e., will LLMs respect copyright information in user input, and behave accordingly? The research problem is critical, as a negative answer would imply that LLMs will become the primary facilitator and accelerator of copyright infringement behavior. We conducted a series of experiments using a diverse set of language models, user prompts, and copyrighted materials, including books, news articles, API documentation, and movie scripts. Our study offers a conservative evaluation of the extent to which language models may infringe upon copyrights when processing user input containing protected material. This research emphasizes the need for further investigation and the importance of ensuring LLMs respect copyright regulations when handling user input to prevent unauthorized use or reproduction of protected content. We also release a benchmark dataset serving as a test bed for evaluating infringement behaviors by LLMs and stress the need for future alignment.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">xu-etal-2024-llms</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Do {LLM}s Know to Respect Copyright Notice?}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xu, Jialiang and Li, Shenglan and Xu, Zhaozhuo and Zhang, Denghui}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Miami, Florida, USA}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2024.emnlp-main.1147/}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2024.emnlp-main.1147}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{20604--20619}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#497174"> <a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">ArXiv</a> </abbr> </div> <div id="xu2024reverse" class="col-sm-8"> <div class="title">Reverse Image Retrieval Cues Parametric Memory in Multimodal LLMs</div> <div class="author"> <em>Jialiang Xu<sup>*</sup></em>, Michael Moor<sup>*</sup>, and Jure Leskovec </div> <div class="periodical"> Nov 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2405.18740" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2405.18740" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Despite impressive advances in recent multimodal large language models (MLLMs), state-of-the-art models such as from the GPT-4 suite still struggle with knowledge-intensive tasks. To address this, we consider Reverse Image Retrieval (RIR) augmented generation, a simple yet effective strategy to augment MLLMs with web-scale reverse image search results. RIR robustly improves knowledge-intensive visual question answering (VQA) of GPT-4V by 37-43%, GPT-4 Turbo by 25-27%, and GPT-4o by 18-20% in terms of open-ended VQA evaluation metrics. To our surprise, we discover that RIR helps the model to better access its own world knowledge. Concretely, our experiments suggest that RIR augmentation helps by providing further visual and textual cues without necessarily containing the direct answer to a query. In addition, we elucidate cases in which RIR can hurt performance and conduct a human evaluation. Finally, we find that the overall advantage of using RIR makes it difficult for an agent that can choose to use RIR to perform better than an approach where RIR is the default setting.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">xu2024reverse</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Reverse Image Retrieval Cues Parametric Memory in Multimodal LLMs}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xu, Jialiang and Moor, Michael and Leskovec, Jure}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2405.18740}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#DC3535"> <a href="https://2024.aclweb.org//" rel="external nofollow noopener" target="_blank">ACL 2024</a> </abbr> <abbr class="badge rounded w-100" style="background-color:#FFAF00"> <div>Award</div> </abbr> </div> <div id="han-etal-2024-word" class="col-sm-8"> <div class="title">Word Embeddings Are Steers for Language Models</div> <div class="author"> Chi Han, <em>Jialiang Xu</em>, Manling Li, Yi Fung, Chenkai Sun, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Nan Jiang, Tarek Abdelzaher, Heng Ji' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, Aug 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Awarded</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.18653/v1/2024.acl-long.864" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://aclanthology.org/2024.acl-long.864.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>ACL 2024 Outstanding Paper Award</p> </div> <div class="abstract hidden"> <p>Language models (LMs) automatically learn word embeddings during pre-training on language corpora. Although word embeddings are usually interpreted as feature vectors for individual words, their roles in language model generation remain underexplored. In this work, we theoretically and empirically revisit output word embeddings and find that their linear transformations are equivalent to steering language model generation styles. We name such steers LM-Steers and find them existing in LMs of all sizes. It requires learning parameters equal to 0.2% of the original LMs‚Äô size for steering each style. On tasks such as language model detoxification and sentiment control, LM-Steers can achieve comparable or superior performance compared with state-of-the-art controlled generation methods while maintaining a better balance with generation quality. The learned LM-Steer serves as a lens in text styles: it reveals that word embeddings are interpretable when associated with language model generations and can highlight text spans that most indicate the style differences. An LM-Steer is transferrable between different language models by an explicit form calculation. One can also continuously steer LMs simply by scaling the LM-Steer or compose multiple LM-Steers by adding their transformations. Our codes are publicly available at https://github.com/Glaciohound/LM-Steer.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">han-etal-2024-word</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Word Embeddings Are Steers for Language Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Han, Chi and Xu, Jialiang and Li, Manling and Fung, Yi and Sun, Chenkai and Jiang, Nan and Abdelzaher, Tarek and Ji, Heng}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">aug</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Bangkok, Thailand}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2024.acl-long.864/}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2024.acl-long.864}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{16410--16430}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#DC3535"> <a href="https://2022.emnlp.org/" rel="external nofollow noopener" target="_blank">EMNLP 2022</a> </abbr> </div> <div id="xu-etal-2022-towards-robust" class="col-sm-8"> <div class="title">Towards Robust Numerical Question Answering: Diagnosing Numerical Capabilities of NLP Systems</div> <div class="author"> <em>Jialiang Xu</em>, Mengyu Zhou, Xinyi He, Shi Han, and Dongmei Zhang </div> <div class="periodical"> <em>In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em>, Dec 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.18653/v1/2022.emnlp-main.542" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://aclanthology.org/2022.emnlp-main.542.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/pdf/emnlp_2022_dnc_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="/assets/pdf/emnlp_2022_dnc_slides.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>Numerical Question Answering is the task of answering questions that require numerical capabilities. Previous works introduce general adversarial attacks to Numerical Question Answering, while not systematically exploring numerical capabilities specific to the topic. In this paper, we propose to conduct numerical capability diagnosis on a series of Numerical Question Answering systems and datasets. A series of numerical capabilities are highlighted, and corresponding dataset perturbations are designed. Empirical results indicate that existing systems are severely challenged by these perturbations. E.g., Graph2Tree experienced a 53.83% absolute accuracy drop against the ‚ÄúExtra‚Äù perturbation on ASDiv-a, and BART experienced 13.80% accuracy drop against the ‚ÄúLanguage‚Äù perturbation on the numerical subset of DROP. As a counteracting approach, we also investigate the effectiveness of applying perturbations as data augmentation to relieve systems‚Äô lack of robust numerical capabilities. With experiment analysis and empirical studies, it is demonstrated that Numerical Question Answering with robust numerical capabilities is still to a large extent an open question. We discuss future directions of Numerical Question Answering and summarize guidelines on future dataset collection and system design.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">xu-etal-2022-towards-robust</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Towards Robust Numerical Question Answering: Diagnosing Numerical Capabilities of {NLP} Systems}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xu, Jialiang and Zhou, Mengyu and He, Xinyi and Han, Shi and Zhang, Dongmei}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Abu Dhabi, United Arab Emirates}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2022.emnlp-main.542/}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2022.emnlp-main.542}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{7950--7966}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%78%6A%6C@%73%74%61%6E%66%6F%72%64.%65%64%75" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://github.com/liamjxu" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/xjl" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="/feed.xml" title="RSS Feed"><i class="fa-solid fa-square-rss"></i></a> <a href="https://scholar.google.com/citations?user=S_mgVngAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://twitter.com/liamjxu" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> </div> <div class="contact-note">The easiest way to reach me would be through email since I regularly check my mailbox every day. </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2025 Jialiang Xu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Profile photo By Ke Xu. Last updated: June 27, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>